<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Research</title>
<link rel="stylesheet" type="text/css" href="css/style.css">
</head>



<body class="container_body">
  <div class="ding">
    <a href="https://www.pku.edu.cn/" target="_blank" ><img src="logo/use_white_all.png" width="320" height="90" float="left" style="margin-top: 55px;margin-bottom: 55px;margin-left: 25px;margin-right: 5px;"></a>
    <img src="logo/myname.png" width="945" height="200" float="right">
  </div>
    
    <hr style="height:10px;border:none;border-top:10px groove skyblue;" /> 
    
    <div class="menu_container">
      <nav>
        <ul>
          <li style="text-align: center;"><a href="homepage.html" class="cond-a">Homepage</a></li>
          <li style="text-align: center; background-color: rgb(255, 255, 255);border-radius: 30px;"><a href="research.html"  class="cond-this">Research</a></li>
          <li style="text-align: center;"><a href="dataset.html" class="cond-a">Dataset</a></li>
          <li style="text-align: center;"><a href="news.html"   class="cond-a">News</a></li>
          <li style="text-align: center;"><a href='publications.html' class="cond-a">Publications</a></li>
          <li style="text-align: center;"><a href="member.html" class="cond-a">Member</a> </li>
        </ul>
      </nav>
    </div>
    <div class="research_title"><br>
        <h1 style="font-family:Microsoft YaHei;">
            Networked Collaborative Intelligence
        </h1>
    </div>
    <div class="research_content"><font size = 5 style="font-family: Lucida Sans Unicode;"><br/>
        1.	<b>Importance of Networked Cooperative Intelligence: </b> With the increasing demand for intelligent ability of agents and the development of communication network, networked cooperative intelligence has become an important method for intelligent agents to perform complex tasks in the future. The performance and robustness of the agents, including vehicles, UAVs and AMRs, etc. can be effectively enhanced by exchanging sensor data and decision results. Of all intelligent tasks, the most fundamental is localization and perception. 
        <br/>
        <div align="center">
          <img border="0" src="images/C-SLAM cover.png" width="800" > 
        </div>
        <br/>
        2.	<b>Cooperative Simultaneous Localization and Mapping(SLAM): </b>In order to interact and manipulate its environment, a robot has to be able to determine its location in space(localization) and information about its environment (mapping)using the data from the on-board sensors. The problems that SLAM faces includes limited perception of single robot, drifts in the large environment, failure in the featureless environment and so on. Our team focuses on getting better localization results[1] and improving the quality of mapping with the help of multi-robot cooperation and communication. Meanwhile, RF information is expected to be applied to our system in order to fuse multi-sensor information and RF space information, which improves the robustness and positioning accuracy. The system prototype has been verified on simulation platform, which is supposed to be transplanted to the AMR in the real world.
        <br/>
        <div align="center">
          <img border="0" src="images/C-SLAM simulation.png" width="800" >
        </div>
        <br/>
        3.	<b>Spatiotemporal Heterogeneous Multimodal Perception: </b>In the process of moving, the mobile agent needs to sense objects in the surrounding environment in real time, so as to realize flexible obstacle avoidance and path planning. However, the perception accuracy and range of a single intelligent agent are greatly limited, especially in harsh weather or complicated environment[2]. Our team focuses on utilizing the perception data from multiple sensing units, including camera, lidar, radar, etc. and multiple intelligent agents to realize spatiotemporal heterogeneous multimodal perception[3]. We also aim to consider the non-ideality of communication and optimize the allocation of network resources. The proposed cooperative perception algorithm has been verified on open source perception dataset, and in-depth research will be based on synesthesia datasets and hardware platforms in out group.
        <br/>
        <div align="center">
          <img border="0" src="images/Spatiotemporal Heterogeneous Multimodal Perception.png" width="800" > 
        </div>
        <br/>
    </font>
  <br><br>
  </div>







  <div style="font-family: Lucida Sans Unicode;width:100%;height:400px;">
    <h1 style="font-family:Microsoft YaHei;">
      Reference
    </h1>
  
    </br>
    <p style="font-size: 20px; width: 1300px;line-height: 2em;text-align: justify;">
      [1] P. Yang, D. Duan, C. Chen, X. Cheng and L. Yang, "Multi-Sensor Multi-Vehicle (MSMV) Localization and Mobility Tracking for Autonomous Driving," in <i>IEEE Transactions on Vehicular Technology</i>, vol. 69, no. 12, pp. 14355-14364, Dec. 2020.
    </p>
    <p style="font-size: 20px; width: 1300px;line-height: 2em;text-align: justify;">
      [2] Y. Li, D. Duan, C. Chen, X. Cheng and L. Yang, "Environmental Sensitivity Evaluation of Neural Networks in Unmanned Vehicle Perception Module," <i>2020 IEEE Wireless Communications and Networking Conference (WCNC)</i>, 2020, pp. 1-6.
    </p>
    <p style="font-size: 20px; width: 1300px;line-height: 2em;text-align: justify;">
      [3] X. Zheng, Y. Li, D. Duan, L. Yang, C. Chen and X. Cheng, "Multi-Vehicle Multi-Sensor Occupancy Grid Maps (MVMS-OGM) for Autonomous Driving," in <i>IEEE Internet of Things Journal</i>, 2022.
    </p>
  
  </div>

  <div class="kong_member" style="margin: auto;height: 100px;"></div>
  <div class="kd_member" style="margin: auto;padding: 5px;">
      <div align="center" style="margin:13px;">
        <img src="logo/use_white_all.png" width="266" height="75" >
      </div>
      <br/>
      <font size = 4 style="font-family:Lucida Sans Unicode; color: aliceblue;">&emsp;&emsp;Copyright Â© Peking University&nbsp;&nbsp;|&nbsp;&nbsp;Address: No.2 Science Building, Peking University, No.5 Yiheyuan Road, Haidian District, Beijing, P.R. China  </font>
  </div>

</body>
</html>